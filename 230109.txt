#실제 데이터와 예측한 데이터의 오차=거리는 마이너스가 될 수 없다= 절대값 또는 제곱 사용-제곱은 너무 커짐 그래서 rmse! 

#수치가 큰 데이터는 log를 씌워줌=데이터가 가운데 쪽으로 모이고 수치도 작아짐=계산이 편해짐=rmsle(kaggle의지표)
#verbose 는 결과들이 반환 된다는 것
19-1
# 소문자가 함수 대문자가 클래스
# earlyStopping의 치명적 단점: 끊은 시점에 weight가 결정(저장), 실질적으로 우리가 원한 지점이 아님
# callback:
# 밀리지 않고 break 한 시점의 weight 저장 = restore_best_weights 옵션을 주면 됨
# patience 몇 번 참는지!
# verbose=1 : earlyStopping 한 지점도 볼 수 있음
# 리스트 형태로 loss와 val_loss 값이 들어간다 이 형태는 dictionary다(키,밸류로 이루어짐=중괄호로 되어있음)
plt.grid() 
# 격자가 들어간다
#2진분류: 두가지로만 분류 - 무조건 sigmoid !(예외 제외) y값이 0과 1로만 정리되어 있어야 함/loss는 binary_crossentropy(구글에 공식 나옴 검색! BCE(x)=




과제!
#마지막 파일(keras 20) accuracy 수정 실수를 정수로(flatten) / 오늘 소스들 최적의 값 전부 정리/18.1 파일 보스톤 한글깨짐 수정